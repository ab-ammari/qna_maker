"""
Application principale Streamlit pour le syst√®me Q&A bas√© sur RAG.
"""
import os
import time
import json
import streamlit as st
import pandas as pd
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Importer les modules d'utilitaires
from utils import (
    DocumentProcessor,
    EmbeddingManager,
    VectorStore,
    LLMHandler,
    VoiceHandler
)

# Constantes
DATA_DIR = "data"
VECTOR_STORE_NAME = "vector_store"

# Initialiser les variables de session
if 'initialized' not in st.session_state:
    st.session_state.initialized = False
    st.session_state.vector_store = None
    st.session_state.document_processor = None
    st.session_state.embedding_manager = None
    st.session_state.llm_handler = None
    st.session_state.voice_handler = None
    st.session_state.documents = []
    st.session_state.processing = False
    st.session_state.last_query = ""
    st.session_state.last_response = ""

def initialize_components():
    """Initialise tous les composants n√©cessaires."""
    if not st.session_state.initialized:
        try:
            # Cr√©er le r√©pertoire de donn√©es s'il n'existe pas
            os.makedirs(DATA_DIR, exist_ok=True)
            
            # Initialiser les composants
            st.session_state.document_processor = DocumentProcessor()
            st.session_state.embedding_manager = EmbeddingManager()
            st.session_state.vector_store = VectorStore()
            
            # Tenter de charger un vector store existant
            if st.session_state.vector_store.load(DATA_DIR, VECTOR_STORE_NAME):
                st.info("Base de connaissances pr√©c√©dente charg√©e avec succ√®s.")
            
            # Initialiser le gestionnaire LLM si la cl√© API est disponible
            api_key = os.environ.get("GROQ_API_KEY")
            if api_key:
                st.session_state.llm_handler = LLMHandler(api_key=api_key)
            
            # Initialiser le gestionnaire vocal
            st.session_state.voice_handler = VoiceHandler()
            
            st.session_state.initialized = True
            return True
        except Exception as e:
            st.error(f"Erreur lors de l'initialisation: {str(e)}")
            return False
    return True

def process_documents(files, urls):
    """Traite les documents et les URLs."""
    st.session_state.processing = True
    processed_docs = []
    
    with st.spinner("Traitement des documents..."):
        # Traiter les fichiers
        for file in files:
            try:
                chunks = st.session_state.document_processor.process_file(file, file.name)
                processed_docs.extend(chunks)
                st.info(f"‚úÖ {file.name} trait√© avec succ√®s ({len(chunks)} chunks)")
            except Exception as e:
                st.error(f"‚ùå Erreur lors du traitement de {file.name}: {str(e)}")
        
        # Traiter les URLs
        for url in urls:
            try:
                chunks = st.session_state.document_processor.process_url(url)
                processed_docs.extend(chunks)
                st.info(f"‚úÖ {url} trait√© avec succ√®s ({len(chunks)} chunks)")
            except Exception as e:
                st.error(f"‚ùå Erreur lors du traitement de {url}: {str(e)}")
    
    # Calculer les embeddings et ajouter au vector store
    if processed_docs:
        with st.spinner("Calcul des embeddings et mise √† jour de la base de connaissances..."):
            texts = [doc["text"] for doc in processed_docs]
            embeddings = st.session_state.embedding_manager.get_embeddings(texts)
            st.session_state.vector_store.add_documents(processed_docs, embeddings)
            
            # Sauvegarder le vector store
            st.session_state.vector_store.save(DATA_DIR, VECTOR_STORE_NAME)
            
            # Mettre √† jour la session
            st.session_state.documents.extend(processed_docs)
            st.success(f"‚úÖ {len(processed_docs)} chunks ajout√©s √† la base de connaissances.")
    
    st.session_state.processing = False

def generate_response(query):
    """G√©n√®re une r√©ponse √† la requ√™te."""
    if not query:
        return "Veuillez entrer une question."
    
    if not st.session_state.vector_store or not st.session_state.llm_handler:
        return "Le syst√®me n'est pas compl√®tement initialis√©. Veuillez v√©rifier vos cl√©s API et r√©essayer."
    
    # Obtenir l'embedding de la requ√™te
    query_embedding = st.session_state.embedding_manager.get_query_embedding(query)
    
    # Rechercher les documents pertinents
    relevant_docs = st.session_state.vector_store.similarity_search(query_embedding, k=4)
    
    if not relevant_docs:
        return "Je n'ai pas trouv√© d'informations pertinentes dans les documents fournis. Veuillez essayer une autre question ou ajouter plus de documents."
    
    # G√©n√©rer la r√©ponse avec le LLM
    response = st.session_state.llm_handler.get_response(query, relevant_docs)
    
    # Mettre √† jour la session
    st.session_state.last_query = query
    st.session_state.last_response = response
    
    return response

def main():
    """Fonction principale de l'application."""
    st.set_page_config(
        page_title="Q&A RAG System",
        page_icon="ü§ñ",
        layout="wide"
    )
    
    st.title("ü§ñ Syst√®me Q&A bas√© sur RAG")
    st.markdown("""
    Bienvenue dans ce syst√®me de questions-r√©ponses bas√© sur RAG (Retrieval-Augmented Generation).
    T√©l√©chargez vos documents ou entrez des URLs pour cr√©er votre base de connaissances personnalis√©e.
    """)
    
    # V√©rifier la cl√© API Groq
    groq_api_key = os.environ.get("GROQ_API_KEY")
    if not groq_api_key:
        api_key = st.text_input("Veuillez entrer votre cl√© API Groq", type="password")
        if api_key:
            os.environ["GROQ_API_KEY"] = api_key
            st.success("Cl√© API Groq enregistr√©e avec succ√®s.")
            if not st.session_state.llm_handler and st.session_state.initialized:
                st.session_state.llm_handler = LLMHandler(api_key=api_key)
            st.experimental_rerun()
    
    # Initialiser les composants
    if not initialize_components():
        st.stop()
    
    # Interface √† onglets
    tab1, tab2, tab3 = st.tabs(["üìö Documents", "‚ùì Questions & R√©ponses", "‚ÑπÔ∏è √Ä propos"])
    
    # Onglet Documents
    with tab1:
        st.header("üìö Gestion des Documents")
        
        # Upload de fichiers
        st.subheader("T√©l√©charger des fichiers")
        uploaded_files = st.file_uploader(
            "T√©l√©chargez des fichiers (PDF, TXT, XLSX)",
            accept_multiple_files=True,
            type=["pdf", "txt", "xlsx", "xls"]
        )
        
        # Entr√©e d'URL
        st.subheader("Ajouter des URLs")
        url_input = st.text_area("Entrez des URLs (une par ligne)")
        urls = [url.strip() for url in url_input.split("\n") if url.strip()]
        
        # Bouton de traitement
        if st.button("Traiter les documents", disabled=st.session_state.processing):
            if not uploaded_files and not urls:
                st.warning("Veuillez t√©l√©charger au moins un fichier ou entrer une URL.")
            else:
                process_documents(uploaded_files, urls)
        
        # Afficher les statistiques
        if st.session_state.documents:
            st.subheader("Statistiques de la base de connaissances")
            
            # Nombre total de documents et de chunks
            sources = set()
            for doc in st.session_state.documents:
                sources.add(doc["metadata"]["source"])
            
            st.metric("Documents sources", len(sources))
            st.metric("Chunks de texte", len(st.session_state.documents))
            
            # Liste des sources
            st.subheader("Sources de donn√©es")
            source_list = list(sources)
            for source in source_list:
                st.write(f"- {source}")
        
        # Bouton pour r√©initialiser la base de connaissances
        if st.session_state.documents and st.button("R√©initialiser la base de connaissances"):
            st.session_state.vector_store = VectorStore()
            st.session_state.documents = []
            
            # Supprimer les fichiers du vector store
            index_path = os.path.join(DATA_DIR, f"{VECTOR_STORE_NAME}.index")
            docs_path = os.path.join(DATA_DIR, f"{VECTOR_STORE_NAME}.pkl")
            
            if os.path.exists(index_path):
                os.remove(index_path)
            if os.path.exists(docs_path):
                os.remove(docs_path)
            
            st.success("Base de connaissances r√©initialis√©e avec succ√®s.")
            st.experimental_rerun()
    
    # Onglet Questions & R√©ponses
    with tab2:
        st.header("‚ùì Questions & R√©ponses")
        
        # V√©rifier si des documents sont charg√©s
        if not st.session_state.documents:
            st.warning("Veuillez d'abord ajouter des documents dans l'onglet 'Documents'.")
            st.stop()
        
        # Interface de question
        st.subheader("Posez votre question")
        
        # Option vocale
        voice_col1, voice_col2 = st.columns([3, 1])
        with voice_col1:
            query = st.text_input("Entrez votre question", key="query_input")
        with voice_col2:
            if st.button("üé§ Parler", key="voice_button"):
                with st.spinner("√âcoute en cours..."):
                    voice_input = st.session_state.voice_handler.recognize_speech()
                    if not voice_input.startswith("Erreur") and not voice_input.startswith("Aucune") and not voice_input.startswith("D√©sol√©"):
                        st.session_state.query_input = voice_input
                        query = voice_input
                        st.experimental_rerun()
                    else:
                        st.error(voice_input)
        
        # Bouton de soumission
        if st.button("Obtenir une r√©ponse", disabled=not query):
            with st.spinner("G√©n√©ration de la r√©ponse en cours..."):
                response = generate_response(query)
                st.session_state.last_response = response
        
        # Afficher la r√©ponse
        if st.session_state.last_query and st.session_state.last_response:
            st.subheader("R√©ponse")
            st.info(f"Question: {st.session_state.last_query}")
            st.write(st.session_state.last_response)
            
            # Option pour lire la r√©ponse √† haute voix
            if st.button("üîä Lire la r√©ponse"):
                st.session_state.voice_handler.text_to_speech(st.session_state.last_response)
    
    # Onglet √Ä propos
    with tab3:
        st.header("‚ÑπÔ∏è √Ä propos de ce syst√®me")
        st.markdown("""
        ### Q&A RAG System
        
        Ce syst√®me utilise l'approche RAG (Retrieval-Augmented Generation) pour cr√©er un assistant de questions-r√©ponses personnalis√© qui s'appuie sur vos propres documents.
        
        #### Technologies utilis√©es :
        - **Document Processing** : LangChain pour traiter divers formats de documents
        - **Embeddings** : Hugging Face pour la g√©n√©ration d'embeddings (gratuit et open-source)
        - **Vector Storage** : FAISS (Facebook AI Similarity Search) pour l'indexation et la recherche vectorielle
        - **LLM** : API Groq pour la g√©n√©ration de r√©ponses (utilisant des mod√®les comme Llama 3 ou Mixtral)
        - **Interface** : Streamlit pour l'interface utilisateur
        - **Vocal** : Fonctionnalit√©s de reconnaissance et de synth√®se vocale
        
        #### Comment √ßa marche :
        1. **T√©l√©chargez vos documents** : PDF, TXT, XLSX ou entrez des URLs
        2. **Le syst√®me traite et indexe** : Les documents sont divis√©s en chunks, transform√©s en vecteurs et index√©s
        3. **Posez vos questions** : Le syst√®me recherche les informations pertinentes dans vos documents
        4. **Obtenez des r√©ponses pr√©cises** : Les r√©ponses sont g√©n√©r√©es √† partir du contexte trouv√©
        
        #### Avantages :
        - **Personnalis√©** : R√©pond aux questions bas√©es sur VOS documents
        - **Pr√©cis** : Les r√©ponses sont ancr√©es dans le contexte fourni, r√©duisant les hallucinations
        - **Gratuit** : Utilise principalement des outils open-source (seule l'API Groq n√©cessite un compte)
        - **Multimodal** : Supporte la voix et le texte
        """)

if __name__ == "__main__":
    main()
